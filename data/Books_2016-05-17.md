name: Re_Engineering_Legacy_Software
isbn: 1617292508
isbn13: 9781617292507
title: [Re-Engineering Legacy Software](http://amzn.com/1617292508)
author: Chris Birchall
publisher: Manning
year: 2015
acquired: 2015-08-06
start: 2016-05-17
stop: 2016-06-30

Ten years prior, I had read
[Working Effectively with Legacy Code](#Working_Effectively_with_Legacy_Code)
by Michael Feathers and I had really liked it.  It had a strong focus on
automated tests and refactoring.  I was hoping this could be an updated take on
the subject, though the table of contents looked more about the human factors
involved in software development.  This was a problem that I was struggling with
at the time: how to modernize a sizable codebase that was too unwieldy for my
small team.

Right off the bat:

> This book is ambitious in scope, setting itself the aim of teaching you
> everything you need to do in order to transform a neglected legacy codebase
> into a maintainable, well-functioning piece of software that can provide value
> to your organization.
> <footer>page xvi</footer>

The book is barely 200 pages long!!  And it's going to cover _everything_?!?
This book is setting itself up to be a disappointment.

Overall, it brings up a lot of good points and ideas of things to try, but fails
to develop any of them to a significant degree.  It might work for project
managers who are not very technical to get a tour of the landscape, pick up some
terms, and have a general understanding of what's needed.  It falls very short
for technical people looking for concrete, actionable items to help them deal
with their legacy software.

It does, however, suggest many avenues to explore on one's own.  I appreciated
the overall sense of automating as many things as possible.  From developers
setting up for a project to continuous integration to static analysis to
deployment, the fewer things that depend on human intervention and arcane
knowledge, the better it is for the project.  It lists quite a few tools for all
these tasks ([Vagrant](https://www.vagrantup.com/),
[Ansible](https://www.ansible.com/),
[FindBugs](http://findbugs.sourceforge.net/), [PMD](http://pmd.github.io/),
[Jenkins](https://jenkins.io/), [Fabric](http://www.fabfile.org/), etc.).  My
personal favorites, such as [Docker](https://www.docker.com/) and microservices,
only get passing mentions.  Some of these tools include trivial examples on how
to set them up.  But any substantial use case is left as an exercise for the
reader.

> First we need to decide what to measure. This depends largely on the
> particular software, but the simple answer is measure everything you can.
> <footer>page 22</footer>

This can be dangerous advice, both politically if measurements are misused or
misinterpreted, and technically as it creates a lot of noise.  People will
optimize for what they are measured by, so beware of what you ask for.

Another important aspect is ensuring information flow through the team.
Keeping it close to the code with `README` files and documentation files in the
same code repository augments the chance that developers will keep them
up-to-date as the code evolves.

I really liked the discussion of splitting a large monolithic application into
services vs. SOA vs. microservices, with decent coverage of the pros and cons of
each.  The section on refactoring code was also a welcomed refresher.  I had
never heard of the Macbeth Syndrome:
_I've come this far, it'll be as hard to finish as to go back_ (not to
be confused with the
[Lady Macbeth effect](https://en.wikipedia.org/wiki/Lady_Macbeth_effect)).

Some of the attempts are humor fall flat and come across as juvenile.

I will end with an interesting quote near the end regarding unit tests:

> If, for example, you can't refactor a piece of code without having to fix or
> rewrite dozens of tests, that may be a sign that your unit tests are becoming
> more trouble than they're worth.
> <footer>page 203</footer>

I have been struggling with this very problem lately, trying to make one change
and having to update over a hundred tests, which is discouraging.  The author
goes so far as to recommend deleting unit tests when you have higher-level tests
to provide coverage.  I'm not completely sold on the idea yet, but it does
resonate.
